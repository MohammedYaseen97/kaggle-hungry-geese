{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "hungrygeese-q-learning-common-sense-knowledge.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LYE-S_U_OM"
      },
      "source": [
        "\n",
        "About the notebook\n",
        "---\n",
        "Submission to the competiton [Hungry Geese](https://www.kaggle.com/c/hungry-geese)\n",
        "\n",
        "Copied from and made (or tried to make) minor edits to [Kaggle.com : HungryGeese: Q-Learning & common sense knowledge](https://www.kaggle.com/victordelafuente/hungrygeese-q-learning-common-sense-knowledge)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "067w_nOsS5VE"
      },
      "source": [
        "# Purpose of this notebook\n",
        "\n",
        "This notebook is just a proof of concept on how to build a simple q-learning agent, so far it doesn't do great (it's improved from the start but still, no winner) but I'll try my best to improve it, if not at least I save someone some ground work... ;-)\n",
        "\n",
        "\n",
        "## Basic QLearner\n",
        "\n",
        "\n",
        "Equation used to update q-table can be seen on image:\n",
        "\n",
        "![Q-Learning](https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686) Source: Wikipedia\n",
        "\n",
        "\n",
        "\n",
        "### Epsilon greedy policy\n",
        "\n",
        "The epsilon greedy policy means at each step we choose the action that yields a higher Q value (estimated reward) from the current state, but a certain epsilon-percent of times we chose a random action (to explore)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fan9UTVS5VI",
        "outputId": "c40154fe-b939-43e4-c7dc-00f01c823406"
      },
      "source": [
        "%%writefile greedy-goose.py\n",
        "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\n",
        "import random as rand\n",
        "from enum import Enum, auto\n",
        "\n",
        "\n",
        "def opposite(action):\n",
        "    if action == Action.NORTH:\n",
        "        return Action.SOUTH\n",
        "    if action == Action.SOUTH:\n",
        "        return Action.NORTH\n",
        "    if action == Action.EAST:\n",
        "        return Action.WEST\n",
        "    if action == Action.WEST:\n",
        "        return Action.EAST\n",
        "    raise TypeError(str(action) + \" is not a valid Action.\")\n",
        "\n",
        "    \n",
        "\n",
        "#Enconding of cell content to build states from observations\n",
        "class CellState(Enum):\n",
        "    EMPTY = 0\n",
        "    FOOD = auto()\n",
        "    GOOSE = auto()\n",
        "\n",
        "\n",
        "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
        "class BornToNotMedalv2:    \n",
        "    def __init__(self):\n",
        "        self.DEBUG=True\n",
        "        self.rows, self.columns = -1, -1        \n",
        "        self.my_index = -1\n",
        "        self.my_head, self.my_tail = -1, -1\n",
        "        self.geese = []\n",
        "        self.heads = []\n",
        "        self.tails = []\n",
        "        self.food = []\n",
        "        self.cell_states = []\n",
        "        self.actions = [action for action in Action]\n",
        "        self.previous_action = None\n",
        "        self.step = 1\n",
        "\n",
        "        \n",
        "    def _adjacent_positions(self, position):\n",
        "        return adjacent_positions(position, self.columns, self.rows)\n",
        " \n",
        "\n",
        "    def _min_distance_to_food(self, position, food=None):\n",
        "        food = food if food!=None else self.food\n",
        "        return min_distance(position, food, self.columns)\n",
        "\n",
        "    \n",
        "    def _row_col(self, position):\n",
        "        return row_col(position, self.columns)\n",
        "    \n",
        "    \n",
        "    def _translate(self, position, direction):\n",
        "        return translate(position, direction, self.columns, self.rows)\n",
        "        \n",
        "        \n",
        "    def preprocess_env(self, observation, configuration):\n",
        "        observation = Observation(observation)\n",
        "        configuration = Configuration(configuration)\n",
        "        \n",
        "        self.rows, self.columns = configuration.rows, configuration.columns        \n",
        "        self.my_index = observation.index\n",
        "        self.hunger_rate = configuration.hunger_rate\n",
        "        self.min_food = configuration.min_food\n",
        "\n",
        "        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n",
        "        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
        "\n",
        "        \n",
        "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n",
        "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
        "        \n",
        "        self.occupied = [p for p in self.geese_cells]\n",
        "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
        "        \n",
        "        \n",
        "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
        "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
        "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
        "        self.food = [f for f in observation.food]\n",
        "        \n",
        "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
        "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
        "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
        "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
        "        self.danger_zone = self.adjacent_to_geese\n",
        "        \n",
        "        #Cell occupation\n",
        "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
        "        for g in self.geese:\n",
        "            for pos in g:\n",
        "                self.cell_states[pos] = CellState.GOOSE.value\n",
        "        for pos in self.heads:\n",
        "                self.cell_states[pos] = CellState.GOOSE.value\n",
        "        for pos in self.my_body:\n",
        "            self.cell_states[pos] = CellState.GOOSE.value\n",
        "                \n",
        "        #detect dead-ends\n",
        "        self.dead_ends = []\n",
        "        for pos_i,_ in enumerate(self.cell_states):\n",
        "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
        "                continue\n",
        "            adjacent = self._adjacent_positions(pos_i)\n",
        "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
        "            num_blocked = sum(adjacent_states)\n",
        "            if num_blocked>=(CellState.GOOSE.value*3):\n",
        "                self.dead_ends.append(pos_i)\n",
        "        \n",
        "        #check for extended dead-ends\n",
        "        new_dead_ends = [pos for pos in self.dead_ends]\n",
        "        while new_dead_ends!=[]:\n",
        "            for pos in new_dead_ends:\n",
        "                self.cell_states[pos]=CellState.GOOSE.value\n",
        "                self.dead_ends.append(pos)\n",
        "            \n",
        "            new_dead_ends = []\n",
        "            for pos_i,_ in enumerate(self.cell_states):\n",
        "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
        "                    continue\n",
        "                adjacent = self._adjacent_positions(pos_i)\n",
        "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
        "                num_blocked = sum(adjacent_states)\n",
        "                if num_blocked>=(CellState.GOOSE.value*3):\n",
        "                    new_dead_ends.append(pos_i)                                    \n",
        "        \n",
        "                \n",
        "    def strategy_random(self, observation, configuration):\n",
        "        if self.previous_action!=None:\n",
        "            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n",
        "        else:\n",
        "            action = rand.choice([action for action in Action])\n",
        "        self.previous_action = action\n",
        "        return action.name\n",
        "                        \n",
        "                        \n",
        "    def safe_position(self, future_position):\n",
        "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
        "    \n",
        "    \n",
        "    def valid_position(self, future_position):\n",
        "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
        "\n",
        "    \n",
        "    def free_position(self, future_position):\n",
        "        return (future_position not in self.occupied) \n",
        "    \n",
        "                        \n",
        "    def strategy_random_avoid_collision(self, observation, configuration):\n",
        "        dead_end_cell = False\n",
        "        free_cell = True\n",
        "        actions = [action \n",
        "                   for action in Action \n",
        "                   for future_position in [self._translate(self.my_head, action)]\n",
        "                   if self.valid_position(future_position)] \n",
        "        if self.previous_action!=None:\n",
        "            actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
        "        if actions==[]:\n",
        "            dead_end_cell = True\n",
        "            actions = [action \n",
        "                       for action in Action \n",
        "                       for future_position in [self._translate(self.my_head, action)]\n",
        "                       if self.free_position(future_position)]\n",
        "            if self.previous_action!=None:\n",
        "                actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
        "            #no alternatives\n",
        "            if actions==[]:\n",
        "                free_cell = False\n",
        "                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n",
        "\n",
        "        action = rand.choice(actions)\n",
        "        self.previous_action = action\n",
        "        if self.DEBUG:\n",
        "            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n",
        "            if free_cell:\n",
        "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n",
        "            else:\n",
        "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n",
        "        return action.name\n",
        "    \n",
        "    \n",
        "    def strategy_greedy_avoid_risk(self, observation, configuration):        \n",
        "        actions = {  \n",
        "            action: self._min_distance_to_food(future_position)\n",
        "            for action in Action \n",
        "            for future_position in [self._translate(self.my_head, action)]\n",
        "            if self.safe_position(future_position)\n",
        "        }\n",
        "  \n",
        "        if self.previous_action!=None:\n",
        "            actions.pop(opposite(self.previous_action), None)\n",
        "        if any(actions):\n",
        "            action = min(actions.items(), key=lambda x: x[1])[0]\n",
        "            self.previous_action = action\n",
        "            if self.DEBUG:\n",
        "                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n",
        "            self.previous_action = action\n",
        "            return action.name\n",
        "        else:\n",
        "            return self.strategy_random_avoid_collision(observation, configuration)\n",
        "    \n",
        "    \n",
        "    #Redefine this method\n",
        "    def agent_strategy(self, observation, configuration):\n",
        "        action = self.strategy_greedy_avoid_risk(observation, configuration)\n",
        "        return action\n",
        "    \n",
        "    \n",
        "    def agent_do(self, observation, configuration):\n",
        "        self.preprocess_env(observation, configuration)\n",
        "        move = self.agent_strategy(observation, configuration)\n",
        "        self.step += 1\n",
        "        #if self.DEBUG:\n",
        "        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n",
        "        return move\n",
        "\n",
        "    \n",
        "    \n",
        "def agent_singleton(observation, configuration):\n",
        "    global gus    \n",
        "    \n",
        "    try:\n",
        "        gus\n",
        "    except NameError:\n",
        "        gus = BornToNotMedalv2()\n",
        "            \n",
        "    action = gus.agent_do(observation, configuration)\n",
        "\n",
        "    \n",
        "    return action"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing greedy-goose.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_1U7w9hS5VS",
        "outputId": "ae3dbeda-989a-4fca-fe4e-b949cc33e863"
      },
      "source": [
        "%%writefile qgoose.py\n",
        "\n",
        "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\n",
        "import random as rand\n",
        "from enum import Enum, auto\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def opposite(action):\n",
        "    if action == Action.NORTH:\n",
        "        return Action.SOUTH\n",
        "    if action == Action.SOUTH:\n",
        "        return Action.NORTH\n",
        "    if action == Action.EAST:\n",
        "        return Action.WEST\n",
        "    if action == Action.WEST:\n",
        "        return Action.EAST\n",
        "    raise TypeError(str(action) + \" is not a valid Action.\")\n",
        "    \n",
        "\n",
        "    \n",
        "#Enconding of cell content to build states from observations\n",
        "class CellState(Enum):\n",
        "    EMPTY = 0\n",
        "    FOOD = auto()\n",
        "    GOOSE = auto()\n",
        "    #search space gets too big too fast... so just 3 cell states\n",
        "    #HEAD = auto()\n",
        "    #BODY = auto()\n",
        "    #TAIL = auto()\n",
        "    \n",
        "    \n",
        "\n",
        "#This class encapsulates a simple qlearning with epsilon-greedy policy, the states and transitions can be defined automatically as we explore (search space is too big to initialize all at once and many states won't be achievable)\n",
        "class QLearner():\n",
        "    def __init__(self, actions, states=None, initial_value=0.1, alpha=0.3, gamma=0.1, epsilon=0.9, create_states_on_exploration=True):\n",
        "        self.actions = actions\n",
        "        self.create_states_on_exploration = create_states_on_exploration\n",
        "        self.initial_value = initial_value\n",
        "        if states!=None:\n",
        "            self.q_table = {\n",
        "                state: [initial_value for _ in self.actions] for state in states\n",
        "            }\n",
        "            self.states = states\n",
        "        else:\n",
        "            self.q_table = dict()\n",
        "            self.states = []\n",
        "            \n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.previous_state = None\n",
        "        self.current_state = None\n",
        "        self.last_action = None\n",
        "        self.last_action_index = None\n",
        "\n",
        "        \n",
        "    def _check_auto_init_state(self, state):\n",
        "        if (state!=None) and (state not in self.q_table.keys()) and self.create_states_on_exploration:\n",
        "            self.q_table[state] = [self.initial_value for _ in self.actions]\n",
        "            self.states.append(state)\n",
        "    \n",
        "\n",
        "    def _epsilon_greedy(self, state):\n",
        "        #create state if needed\n",
        "        self._check_auto_init_state(state)\n",
        "        \n",
        "        if (rand.random() < self.epsilon):\n",
        "            action = rand.choice(self.actions)\n",
        "            self.last_action_index = self.actions.index(action)\n",
        "        else:\n",
        "            q_state = self.q_table[state]\n",
        "            max_val = max(q_state)\n",
        "            self.last_action_index = rand.choice([i for i,v in enumerate(q_state) if v==max_val])\n",
        "            action = self.actions[self.last_action_index]\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def process_reward(self, reward, previous_state=None, last_action=None, last_action_index=None):\n",
        "        if previous_state==None:\n",
        "            previous_state = self.previous_state\n",
        "        if last_action==None:\n",
        "            last_action = self.last_action\n",
        "        if last_action_index==None:\n",
        "            last_action_index = self.last_action_index\n",
        "            \n",
        "        if (previous_state==None) or (last_action==None):\n",
        "            return\n",
        "        \n",
        "        #create state if needed\n",
        "        self._check_auto_init_state(previous_state)\n",
        "        \n",
        "        q = self.q_table\n",
        "        q_old = q[previous_state][last_action_index]\n",
        "        next_state = self.current_state\n",
        "        if next_state!=None:        \n",
        "            best_scenario = q[next_state].index(max(q[next_state]))\n",
        "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.gamma * best_scenario - q_old)\n",
        "        else:\n",
        "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.initial_value - q_old)\n",
        "\n",
        "            \n",
        "    def epsilon_greedy_choose_action(self, state):\n",
        "        self.previous_state = self.current_state\n",
        "        self.current_state = state\n",
        "        self.last_action = self._epsilon_greedy(state)\n",
        "        return self.last_action\n",
        "    \n",
        "    \n",
        "    def reset_internal_states(self):\n",
        "        self.previous_state = None\n",
        "        self.current_state = None\n",
        "        self.last_action = None\n",
        "        self.last_action_index = None\n",
        "          \n",
        "            \n",
        "    def save_pickle(self, name):\n",
        "        save_data = (self.actions,\n",
        "                     self.q_table,\n",
        "                     self.states,\n",
        "                     )\n",
        "        with open(f'{name}', 'wb') as handle:\n",
        "            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            \n",
        "    def load_pickle(self, name):\n",
        "        with open(f'{name}', 'rb') as handle:\n",
        "            data = pickle.load(handle)\n",
        "            self.actions, self.q_table, self.states = data\n",
        "            \n",
        "            \n",
        "\n",
        "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
        "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
        "class BornToNotMedalv2:    \n",
        "    def __init__(self):\n",
        "        self.DEBUG=True\n",
        "        self.rows, self.columns = -1, -1        \n",
        "        self.my_index = -1\n",
        "        self.my_head, self.my_tail = -1, -1\n",
        "        self.geese = []\n",
        "        self.heads = []\n",
        "        self.tails = []\n",
        "        self.food = []\n",
        "        self.cell_states = []\n",
        "        self.actions = [action for action in Action]\n",
        "        self.previous_action = None\n",
        "        self.step = 1\n",
        "\n",
        "        \n",
        "    def _adjacent_positions(self, position):\n",
        "        return adjacent_positions(position, self.columns, self.rows)\n",
        " \n",
        "\n",
        "    def _min_distance_to_food(self, position, food=None):\n",
        "        food = food if food!=None else self.food\n",
        "        return min_distance(position, food, self.columns)\n",
        "\n",
        "    \n",
        "    def _row_col(self, position):\n",
        "        return row_col(position, self.columns)\n",
        "    \n",
        "    \n",
        "    def _translate(self, position, direction):\n",
        "        return translate(position, direction, self.columns, self.rows)\n",
        "        \n",
        "        \n",
        "    def preprocess_env(self, observation, configuration):\n",
        "        observation = Observation(observation)\n",
        "        configuration = Configuration(configuration)\n",
        "        \n",
        "        self.rows, self.columns = configuration.rows, configuration.columns        \n",
        "        self.my_index = observation.index\n",
        "        self.hunger_rate = configuration.hunger_rate\n",
        "        self.min_food = configuration.min_food\n",
        "\n",
        "        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n",
        "        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
        "\n",
        "        \n",
        "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n",
        "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
        "        \n",
        "        self.occupied = [p for p in self.geese_cells]\n",
        "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
        "        \n",
        "        \n",
        "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
        "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
        "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
        "        self.food = [f for f in observation.food]\n",
        "        \n",
        "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
        "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
        "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
        "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
        "        self.danger_zone = self.adjacent_to_geese\n",
        "        \n",
        "        #Cell occupation\n",
        "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
        "        for g in self.geese:\n",
        "            for pos in g:\n",
        "                self.cell_states[pos] = CellState.GOOSE.value\n",
        "        for pos in self.heads:\n",
        "                self.cell_states[pos] = CellState.GOOSE.value\n",
        "        for pos in self.my_body:\n",
        "            self.cell_states[pos] = CellState.GOOSE.value\n",
        "                \n",
        "        #detect dead-ends\n",
        "        self.dead_ends = []\n",
        "        for pos_i,_ in enumerate(self.cell_states):\n",
        "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
        "                continue\n",
        "            adjacent = self._adjacent_positions(pos_i)\n",
        "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
        "            num_blocked = sum(adjacent_states)\n",
        "            if num_blocked>=(CellState.GOOSE.value*3):\n",
        "                self.dead_ends.append(pos_i)\n",
        "        \n",
        "        #check for extended dead-ends\n",
        "        new_dead_ends = [pos for pos in self.dead_ends]\n",
        "        while new_dead_ends!=[]:\n",
        "            for pos in new_dead_ends:\n",
        "                self.cell_states[pos]=CellState.GOOSE.value\n",
        "                self.dead_ends.append(pos)\n",
        "            \n",
        "            new_dead_ends = []\n",
        "            for pos_i,_ in enumerate(self.cell_states):\n",
        "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
        "                    continue\n",
        "                adjacent = self._adjacent_positions(pos_i)\n",
        "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
        "                num_blocked = sum(adjacent_states)\n",
        "                if num_blocked>=(CellState.GOOSE.value*3):\n",
        "                    new_dead_ends.append(pos_i)                                    \n",
        "        \n",
        "                \n",
        "    def strategy_random(self, observation, configuration):\n",
        "        if self.previous_action!=None:\n",
        "            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n",
        "        else:\n",
        "            action = rand.choice([action for action in Action])\n",
        "        self.previous_action = action\n",
        "        return action.name\n",
        "                        \n",
        "                        \n",
        "    def safe_position(self, future_position):\n",
        "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
        "    \n",
        "    \n",
        "    def valid_position(self, future_position):\n",
        "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
        "\n",
        "    \n",
        "    def free_position(self, future_position):\n",
        "        return (future_position not in self.occupied) \n",
        "    \n",
        "                        \n",
        "    def strategy_random_avoid_collision(self, observation, configuration):\n",
        "        dead_end_cell = False\n",
        "        free_cell = True\n",
        "        actions = [action \n",
        "                   for action in Action \n",
        "                   for future_position in [self._translate(self.my_head, action)]\n",
        "                   if self.valid_position(future_position)] \n",
        "        if self.previous_action!=None:\n",
        "            actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
        "        if actions==[]:\n",
        "            dead_end_cell = True\n",
        "            actions = [action \n",
        "                       for action in Action \n",
        "                       for future_position in [self._translate(self.my_head, action)]\n",
        "                       if self.free_position(future_position)]\n",
        "            if self.previous_action!=None:\n",
        "                actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
        "            #no alternatives\n",
        "            if actions==[]:\n",
        "                free_cell = False\n",
        "                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n",
        "\n",
        "        action = rand.choice(actions)\n",
        "        self.previous_action = action\n",
        "        if self.DEBUG:\n",
        "            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n",
        "            if free_cell:\n",
        "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n",
        "            else:\n",
        "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n",
        "        return action.name\n",
        "    \n",
        "    \n",
        "    def strategy_greedy_avoid_risk(self, observation, configuration):        \n",
        "        actions = {  \n",
        "            action: self._min_distance_to_food(future_position)\n",
        "            for action in Action \n",
        "            for future_position in [self._translate(self.my_head, action)]\n",
        "            if self.safe_position(future_position)\n",
        "        }\n",
        "  \n",
        "        if self.previous_action!=None:\n",
        "            actions.pop(opposite(self.previous_action), None)\n",
        "        if any(actions):\n",
        "            action = min(actions.items(), key=lambda x: x[1])[0]\n",
        "            self.previous_action = action\n",
        "            if self.DEBUG:\n",
        "                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n",
        "            self.previous_action = action\n",
        "            return action.name\n",
        "        else:\n",
        "            return self.strategy_random_avoid_collision(observation, configuration)\n",
        "    \n",
        "    \n",
        "    #Redefine this method\n",
        "    def agent_strategy(self, observation, configuration):\n",
        "        action = self.strategy_greedy_avoid_risk(observation, configuration)\n",
        "        return action\n",
        "    \n",
        "    \n",
        "    def agent_do(self, observation, configuration):\n",
        "        self.preprocess_env(observation, configuration)\n",
        "        move = self.agent_strategy(observation, configuration)\n",
        "        self.step += 1\n",
        "        #if self.DEBUG:\n",
        "        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n",
        "        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n",
        "        return move\n",
        "\n",
        "\n",
        "        \n",
        "#This is our Q-Learning Hungry Geese Agent\n",
        "class QGoose(BornToNotMedalv2, QLearner):    \n",
        "    def __init__(self):\n",
        "        self.POV_DISTANCE=3\n",
        "        BornToNotMedalv2.__init__(self)\n",
        "        QLearner.__init__(self, self.actions, initial_value=0.01, alpha=0.1, gamma=.8, epsilon=0.1)\n",
        "        self.world = None\n",
        "        self.previous_length = 0\n",
        "        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n",
        "       \n",
        "    \n",
        "    def title_state_from_row_col(self, row, col):\n",
        "        pos = self.columns*row+col-1\n",
        "        if pos in self.heads or pos==self.my_head:\n",
        "            return CellState.GOOSE\n",
        "        elif pos in self.bodies or pos==self.my_body:\n",
        "            return CellState.GOOSE\n",
        "        elif pos in self.tails or pos==self.my_tail:\n",
        "            return CellState.GOOSE\n",
        "        elif pos in self.food:\n",
        "            return CellState.FOOD\n",
        "        else:\n",
        "            return CellState.EMPTY\n",
        "    \n",
        "    \n",
        "    def state_from_world(self):\n",
        "        state = []\n",
        "        row_0, col_0 = self._row_col(self.my_head)\n",
        "        for col_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
        "            for row_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
        "                row_i = (row_0+row_delta)%self.rows\n",
        "                col_i = (col_0+col_delta)%self.columns\n",
        "                state.append(self.title_state_from_row_col(row_i, col_i))\n",
        "        state = \"\".join([str(s.value) for s in state])\n",
        "        return state\n",
        "    \n",
        "    \n",
        "    def common_sense_after_move_choosen(self, action):\n",
        "        future_position = self._translate(self.my_head, action)\n",
        "  \n",
        "        if future_position in self.occupied:\n",
        "            return -10 \n",
        "        elif self.previous_action==opposite(self.last_action): #opposite is currently a patch until Action.opposite works...\n",
        "            return -10\n",
        "        elif self.previous_action in self.dead_ends:\n",
        "            return -1\n",
        "        else:\n",
        "            min_distance_to_food = self._min_distance_to_food(future_position)\n",
        "            aux_last = self.last_min_distance_to_food\n",
        "            self.last_min_distance_to_food=min_distance_to_food\n",
        "            \n",
        "            if min_distance_to_food<aux_last:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "    \n",
        "    \n",
        "    def agent_strategy(self, observation, configuration):\n",
        "        state = self.state_from_world()\n",
        "        \n",
        "        #Process reward for growing\n",
        "        reward = len(self.my_body)+2*self.step #Geese really like pizza!!! 8-)\n",
        "        self.previous_length = len(self.my_body)\n",
        "        self.process_reward(reward)\n",
        "        \n",
        "        #Choose action\n",
        "        action = self.epsilon_greedy_choose_action(state)\n",
        "        \n",
        "        #Apply some common sense like colliding is bad... ;-)\n",
        "        cs_reward = self.common_sense_after_move_choosen(action)\n",
        "        if cs_reward<0:\n",
        "            #update q-table\n",
        "            self.process_reward(reward, previous_state=state, last_action=action, last_action_index=self.actions.index(action))\n",
        "\n",
        "            #choose new greedy risk averse valid action\n",
        "            random_action = self.strategy_greedy_avoid_risk(observation, configuration)                                   \n",
        "            #update internal action attributes\n",
        "            aux = [(action,index) for index,action in enumerate(Action) if action.name==random_action][0]\n",
        "            self.last_action = aux[0]\n",
        "            self.last_action_index = aux[1]\n",
        "            action = self.last_action\n",
        "        print(f'q-agent q_table{self.q_table}', flush=True)\n",
        "        \n",
        "        self.previous_action = action    \n",
        "        return Action(action).name\n",
        "\n",
        "\n",
        "        \n",
        "def agent_singleton(observation, configuration):\n",
        "    global gus    \n",
        "    saved=\"qgoose.pickle\"\n",
        "    \n",
        "    try:\n",
        "        gus\n",
        "    except NameError:\n",
        "        gus = QGoose()\n",
        "        if os.path.isfile(saved) and os.stat(saved).st_size>0:\n",
        "            if gus.DEBUG:\n",
        "                print(\"Loading agent, q-table...\")\n",
        "            gus.load_pickle(saved)\n",
        "            if gus.DEBUG:\n",
        "                print(\"Loaded!\")\n",
        "        elif gus.DEBUG:\n",
        "            print(\"No previous trained QTable found!\")\n",
        "            \n",
        "    action = gus.agent_do(observation, configuration)\n",
        "    #print(\"Saving QGoose pickle!!!\", saved)\n",
        "    gus.save_pickle(saved)\n",
        "    \n",
        "    return action"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing qgoose.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyV5irX5S5Ve"
      },
      "source": [
        "Now just to check it's working. \n",
        "\n",
        "Let the agent play some games and try to learn against greedy, this takes a while... (results not displayed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz2fOet4TedL",
        "outputId": "1f05f54d-3a72-46a1-c86f-7d2db977781e"
      },
      "source": [
        "!pip install kaggle_environments"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle_environments\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/88/d54f1ced9fa3c64bfc95c9838d23c3926258e82a15f57c4fbe84524e1624/kaggle_environments-1.7.11-py2.py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.6MB/s \n",
            "\u001b[?25hCollecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (3.7.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (54.1.2)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (0.17.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle_environments) (20.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kaggle_environments) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kaggle_environments) (3.4.1)\n",
            "\u001b[31mERROR: nbclient 0.5.3 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonschema, kaggle-environments\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "Successfully installed jsonschema-3.2.0 kaggle-environments-1.7.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3GNGSdNS5Vf",
        "outputId": "21aae44e-71ee-435f-ebb3-28fe210f536d"
      },
      "source": [
        "import kaggle_environments\n",
        "from kaggle_environments import make\n",
        "from tqdm import tqdm\n",
        "\n",
        "GAMES = 1500\n",
        "USE_TQDM = False\n",
        "\n",
        "env = make(\"hungry_geese\", debug=False)\n",
        "games = tqdm(range(GAMES)) if USE_TQDM else range(GAMES)\n",
        "for i in games:\n",
        "    env.run([\n",
        "        \"qgoose.py\",\n",
        "        \"greedy-goose.py\", \n",
        "        \"greedy-goose.py\", \n",
        "        \"greedy-goose.py\", \n",
        "    ])\n",
        "    if i%100==0:\n",
        "        print(str(i) + ' games executed')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading environment football failed: No module named 'gfootball'\n",
            "0 games executed\n",
            "100 games executed\n",
            "200 games executed\n",
            "300 games executed\n",
            "400 games executed\n",
            "500 games executed\n",
            "600 games executed\n",
            "700 games executed\n",
            "800 games executed\n",
            "900 games executed\n",
            "1000 games executed\n",
            "1100 games executed\n",
            "1200 games executed\n",
            "1300 games executed\n",
            "1400 games executed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q4Ce0UAZPhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9e7bc7-3747-4f93-86c5-cf420792dc62"
      },
      "source": [
        "from kaggle_environments import evaluate\n",
        "\n",
        "NUM_EPISODES = 10\n",
        "evaluate(\"hungry_geese\",\n",
        "    [\n",
        "        \"qgoose.py\",\n",
        "        \"greedy-goose.py\", \n",
        "        \"greedy-goose.py\", \n",
        "        \"greedy-goose.py\", \n",
        "    ],\n",
        "    num_episodes=NUM_EPISODES,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3401, 11923, 11814, 11813],\n",
              " [7305, 3807, 7307, 7415],\n",
              " [8303, 9922, 10014, 7205],\n",
              " [8702, 18021, 11818, 17925],\n",
              " [4001, 8711, 13421, 13416],\n",
              " [11405, 11523, 6012, 3706],\n",
              " [7003, 8808, 8813, 8911],\n",
              " [17012, 6909, 4909, 17122],\n",
              " [6509, 9614, 12220, 12115],\n",
              " [13311, 17819, 5607, 17918]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDV28pI0Gcep"
      },
      "source": [
        "%cp qgoose.py main.py"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlI-WpqpYWEH"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Since we need the trained data in the form of a pickle file, we encapsulate both in a tar file, and submit it directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FWccR2VGgy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca39366-0b27-4c35-8ffb-c7e0a2b63043"
      },
      "source": [
        "!tar cvzf submission.tar.gz main.py qgoose.pickle "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main.py\n",
            "qgoose.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zjD2PpfS5Vf"
      },
      "source": [
        "# Victor's Conclusions\n",
        "\n",
        "Ok, so far the greedy agent with hard coded rules works better and doesn't need any training time...\n",
        "\n",
        "* The q-learning agent needs to find by itself more common sense knowledge that the other agent has beforehand (move towards food, ...)\n",
        "* Already added some common sense: collision and opposite direction moves negative rewards, collision avoidance\n",
        "* Sometimes greedy just ends crashing with itself\n",
        "* Q-Agent mostly randomly browses :-(\n",
        "\n",
        "# What's next?\n",
        "* Adding information about closest food and/or agents to the state definitions?\n",
        "* Stocastic estimation from current transition to set of \"next states seen from current\"\n",
        "* Tuning hyperparameters\n",
        "* Choosing other technique? xD\n",
        "\n",
        "# My Findings\n",
        "\n",
        "* Verified by me, the greedy agent wins hands down.\n",
        "* I tried to improve on the performance by changing the reward to simple reward when the goose eats the food rather than the moving closer to food. Turns out Victor had already used it, and enhanced it to the latter for better performance. \n",
        "* The q-learning agent needs to be trained longer.. i feel it's just the sheer volume of the states that causes the agent to move randomly.. training it for longer than 500 games (originally used by victor) improves its performance. But it's a pain to accomplish.\n",
        "\n",
        "# To watch a game\n",
        "\n",
        "Currently doesn't save (timeout error on notebook save inside Kaggle) but you can run the notebook yourself (just uncomment the following block of code) (Runs on kaggle kernel on chrome)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X3vd-5avS5Vr"
      },
      "source": [
        "# from kaggle_environments import make\n",
        "# env = make(\"hungry_geese\", debug=False)\n",
        "# env.run(\n",
        "#     [\n",
        "#         \"qgoose.py\", \n",
        "#         \"greedy-goose.py\",\n",
        "#     ],  \n",
        "# )\n",
        "# env.render(mode=\"ipython\", width=800, height=700) #takes too long and hangs firefox many times inside nootebook, no idea why... :-("
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpVGv3UyS5Vs"
      },
      "source": [
        "If you need to \"reset\" training while on notebook uncomment the following"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nroNqtAKS5Vs"
      },
      "source": [
        "%%script bash\n",
        "# rm qgoose.pickle"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uncEXLqfS5Vt"
      },
      "source": [
        "*Thanks for checking this out!*"
      ]
    }
  ]
}